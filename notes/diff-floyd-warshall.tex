
\section{Diff FloydWarshall}
Over simplified. Ignoring the cost of going through the entire state space:
    \begin{algorithm}
      \KwData{Graph $\graph_0 = (\vtces, \edges)$\;}
      Initialize $\fwcost(\state_i, \act_i, \state_j; \param_{\fwcost}) = 100$ \;
      Initialize $\qValue(\state_i,\act_i; \param_{\qValue}) = 1$ \;
      Initialize $\vma = 0.1$, $\prewma = 0.9$ \;
      Let minimum path cost $\minedgecost = 0.05$ \;
      Observe $\meas_0$ from environment \;
      $\state_0 = \ObsEnc(\meas_0; \param_E)$ \;
      \For{$t \leftarrow 1$ \KwTo $\epiT$}{
        Take action $\act_{t-1}$\;
        Observe $\meas_t$, $\rew_t$\;
        Encode state $\state_t = \ObsEnc(\measurements; \param_E)$\;
        \tcc{Initialize new FW values}
        $\fwcost(\state, \act, \state_{t})
        = \min \{
               \fwcost(\state, \act, \state_t),
                \fwcost(\state, \act, \state_{t-1}) + \minedgecost
            \}
        \qquad \forall \state \in \State, \act \in \Action$ \;
        \tcc{Q-Value update}
        $\qValue(\state_{t-1}, \act_{t-1}) = (1-\qma) (\rew_t + \discount \max_{\act_k}\qValue(\state_t, \act_k)) + \qma \qValue(\state_{t-1}, \act_{t-1})$\;
        \If{$\state_t$ is visited the first time}{
            \For{$(\state_i, \state_k, \act_k) \in (\State \times \State \times \Action)$}{
                \tcc{Run the Floyd Warshall update}
                $\fwcost(\state_k, \act_k, \state_i) =
                \min \{
                    \fwcost(\state_k, \act_k, \state_i),
                    \fwcost(\state_k, \act_k, \state_t) + \min_{\act \in \Action}\fwcost(\state_t, \act, \state_i)
                \}$
                \;

                $\qValue(\state_k, \act_k) = \max \{
                        \qValue(\state_k, \act_k),
                        \max_{\act} \qValue(\state_i, \act) - \fwcost(\state_k, \act_k, \state_i)
                        \}$
                    \;

            }
        }
      }
      \KwResult{To follow the shortest path $\state_i$ to $\state_j$, follow the
        neighbors with highest $\qValue$\;
        $\policy(\state_k) = \arg \max_{\act_k \in \Action} \qValue(\state_k, \act_k)$\;
      }
      \caption{\small How to solve small windy grid world with randomized goals?}
      \label{alg:floyd-warshall-small}
    \end{algorithm}

What do we learn from this?

\subsection{Pros and Cons}
\begin{tabular}{p{0.5\textwidth}p{0.5\textwidth}}
  \toprule
  Pros & Cons \\
  \midrule
\begin{enumerate}
  \item Some problems might need decomposition of short term value of the state space vs long term value of the state space.
  \item Temporal decomposition of value space.
  \item Replaces discount factor with a more meaningful attribute.
\end{enumerate}
 & 
\begin{enumerate}
  \item It is slow
\end{enumerate}

\\ \bottomrule
\end{tabular}
